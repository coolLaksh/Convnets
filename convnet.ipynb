{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Architecture OverView:\n",
    "\n",
    "\n",
    "        INPUT - CONV1 - RELU - CONV2 - RELU - MAXPOOL - FULLY_CONNECTED_LAYER - OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "NUM_OUTPUT = 10\n",
    "LEARNING_RATE = 0.03\t#learning rate\n",
    "IMG_WIDTH = 28\n",
    "IMG_DEPTH = 1\n",
    "FILTER_SIZE=5\n",
    "NUM_FILT1 = 8\n",
    "NUM_FILT2 = 8\n",
    "BATCH_SIZE = 50\n",
    "NUM_EPOCHS = 2\t # number of iterations\n",
    "MU = 0.95\n",
    "PICKLE_FILE = \"trained.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Returns idexes of maximum value of the array\n",
    "def nanargmax(a):\n",
    "\tidx = np.argmax(a, axis=None)\n",
    "\tmulti_idx = np.unravel_index(idx, a.shape)\n",
    "\tif np.isnan(a[multi_idx]):\n",
    "\t\tnan_count = np.sum(np.isnan(a))\n",
    "\t\t# In numpy < 1.8 use idx = np.argsort(a, axis=None)[-nan_count-1]\n",
    "\t\tidx = np.argpartition(a, -nan_count-1, axis=None)[-nan_count-1]\n",
    "\t\tmulti_idx = np.unravel_index(idx, a.shape)\n",
    "\treturn multi_idx\n",
    "\n",
    "def maxpool(X, f, s):\n",
    "    l, w, h = X.shape\n",
    "    pool = np.zeros((l, (w-f)//s + 1, (h-f)//s + 1))  # Use // for floor division\n",
    "    for channel in range(0, l):\n",
    "        row = 0\n",
    "        for x in range(0, w-f+1, s):\n",
    "            col = 0\n",
    "            for y in range(0, h-f+1, s):\n",
    "                pool[channel, row, col] = np.max(X[channel, x:x+f, y:y+f])\n",
    "                col += 1\n",
    "            row += 1\n",
    "    return pool\n",
    "\n",
    "\n",
    "def softmax_cost(out,y):\n",
    "\teout = np.exp(out)#we dont have 128 a typo fixed\n",
    "\tprobs = eout/sum(eout)\n",
    "\t\n",
    "\tp = sum(y*probs)\n",
    "\tcost = -np.log(p)\t## (Only data loss. No regularised loss)\n",
    "\treturn cost,probs\t\n",
    "\n",
    "\n",
    "## Returns gradient for all the paramaters in each iteration\n",
    "def ConvNet(image, label, filt1, filt2, bias1, bias2, theta3, bias3):\n",
    "\t#####################################################################################################################\n",
    "\t#######################################  Feed forward to get all the layers  ########################################\n",
    "\t#####################################################################################################################\n",
    "\n",
    "\t## Calculating first Convolution layer\n",
    "\t\t\n",
    "\t## l - channel\n",
    "\t## w - size of square image\n",
    "\t## l1 - No. of filters in Conv1\n",
    "\t## l2 - No. of filters in Conv2\n",
    "\t## w1 - size of image after conv1\n",
    "\t## w2 - size of image after conv2\n",
    "\t(l, w, w) = image.shape\t\t\n",
    "\tl1 = len(filt1)\n",
    "\tl2 = len(filt2)\n",
    "\t( _, f, f) = filt1[0].shape\n",
    "\tw1 = w-f+1\n",
    "\tw2 = w1-f+1\n",
    "\t\n",
    "\tconv1 = np.zeros((l1,w1,w1))\n",
    "\tconv2 = np.zeros((l2,w2,w2))\n",
    "\n",
    "\tfor jj in range(0,l1):\n",
    "\t\tfor x in range(0,w1):\n",
    "\t\t\tfor y in range(0,w1):\n",
    "\t\t\t\tconv1[jj,x,y] = np.sum(image[:,x:x+f,y:y+f]*filt1[jj])+bias1[jj]\n",
    "\tconv1[conv1<=0] = 0 #relu activation\n",
    "\n",
    "\t## Calculating second Convolution layer\n",
    "\tfor jj in range(0,l2):\n",
    "\t\tfor x in range(0,w2):\n",
    "\t\t\tfor y in range(0,w2):\n",
    "\t\t\t\tconv2[jj,x,y] = np.sum(conv1[:,x:x+f,y:y+f]*filt2[jj])+bias2[jj]\n",
    "\tconv2[conv2<=0] = 0 # relu activation\n",
    "\n",
    "\t## Pooled layer with 2*2 size and stride 2,2\n",
    "\tpooled_layer = maxpool(conv2, 2, 2)\t\n",
    "\n",
    "\tfc1 = pooled_layer.reshape((int(w2/2)*int(w2/2)*l2,1))\n",
    "\t\n",
    "\tout = theta3.dot(fc1) + bias3\t#10*1\n",
    "\t\n",
    "\t######################################################################################################################\n",
    "\t########################################  Using softmax function to get cost  ########################################\n",
    "\t######################################################################################################################\n",
    "\tcost, probs = softmax_cost(out, label.astype(float))\n",
    "\tif np.argmax(out)==np.argmax(label):\n",
    "\t\tacc=1\n",
    "\telse:\n",
    "\t\tacc=0\n",
    "\t#######################################################################################################################\n",
    "\t##########################  Backpropagation to get gradient\tusing chain rule of differentiation  ######################\n",
    "\t#######################################################################################################################\n",
    "\tdout = probs - label\t#\tdL/dout\n",
    "\t\n",
    "\tdtheta3 = dout.dot(fc1.T) \t\t##\tdL/dtheta3\n",
    "\n",
    "\tdbias3 = sum(dout.T).T.reshape((10,1))\t\t##\tdbias3\t\n",
    "\n",
    "\tdfc1 = theta3.T.dot(dout)\t\t##\tdL/dfc1\n",
    "\n",
    "\tdpool = dfc1.T.reshape((l2, int(w2/2),int(w2/2)))\n",
    "\n",
    "\tdconv2 = np.zeros((l2, w2, w2))\n",
    "\t\n",
    "\tfor jj in range(0,l2):\n",
    "\t\ti=0\n",
    "\t\twhile(i<w2):\n",
    "\t\t\tj=0\n",
    "\t\t\twhile(j<w2):\n",
    "\t\t\t\t(a,b) = nanargmax(conv2[jj,i:i+2,j:j+2]) ## Getting indexes of maximum value in the array\n",
    "\t\t\t\tdconv2[jj,i+a,j+b] = dpool[jj,int(i/2),int(j/2)]\n",
    "\t\t\t\tj+=2\n",
    "\t\t\ti+=2\n",
    "\t\n",
    "\tdconv2[conv2<=0]=0\n",
    "\n",
    "\tdconv1 = np.zeros((l1, w1, w1))\n",
    "\tdfilt2 = {}\n",
    "\tdbias2 = {}\n",
    "\tfor xx in range(0,l2):\n",
    "\t\tdfilt2[xx] = np.zeros((l1,f,f))\n",
    "\t\tdbias2[xx] = 0\n",
    "\n",
    "\tdfilt1 = {}\n",
    "\tdbias1 = {}\n",
    "\tfor xx in range(0,l1):\n",
    "\t\tdfilt1[xx] = np.zeros((l,f,f))\n",
    "\t\tdbias1[xx] = 0\n",
    "\n",
    "\tfor jj in range(0,l2):\n",
    "\t\tfor x in range(0,w2):\n",
    "\t\t\tfor y in range(0,w2):\n",
    "\t\t\t\tdfilt2[jj]+=dconv2[jj,x,y]*conv1[:,x:x+f,y:y+f]\n",
    "\t\t\t\tdconv1[:,x:x+f,y:y+f]+=dconv2[jj,x,y]*filt2[jj]\n",
    "\t\tdbias2[jj] = np.sum(dconv2[jj])\n",
    "\tdconv1[conv1<=0]=0\n",
    "\tfor jj in range(0,l1):\n",
    "\t\tfor x in range(0,w1):\n",
    "\t\t\tfor y in range(0,w1):\n",
    "\t\t\t\tdfilt1[jj] += dconv1[jj, x, y] * image[:, x:x+f, y:y+f].astype(float)\n",
    "\n",
    "\t\tdbias1[jj] = np.sum(dconv1[jj])\n",
    "\n",
    "\t\n",
    "\treturn [dfilt1, dfilt2, dbias1, dbias2, dtheta3, dbias3, cost, acc]\n",
    "\n",
    "\n",
    "def initialize_param(f, l):\n",
    "\treturn 0.01*np.random.rand(l, f, f)\n",
    "\n",
    "def initialize_theta(NUM_OUTPUT, l_in):\n",
    "\treturn 0.01*np.random.rand(NUM_OUTPUT, l_in)\n",
    "\n",
    "def initialise_param_lecun_normal(FILTER_SIZE, IMG_DEPTH, scale=1.0, distribution='normal'):\n",
    "\t\n",
    "    if scale <= 0.:\n",
    "            raise ValueError('`scale` must be a positive float. Got:', scale)\n",
    "\n",
    "    distribution = distribution.lower()\n",
    "    if distribution not in {'normal'}:\n",
    "        raise ValueError('Invalid `distribution` argument: '\n",
    "                             'expected one of {\"normal\", \"uniform\"} '\n",
    "                             'but got', distribution)\n",
    "\n",
    "    scale = scale\n",
    "    distribution = distribution\n",
    "    fan_in = FILTER_SIZE*FILTER_SIZE*IMG_DEPTH\n",
    "    scale = scale\n",
    "    stddev = scale * np.sqrt(1./fan_in)\n",
    "    shape = (IMG_DEPTH,FILTER_SIZE,FILTER_SIZE)\n",
    "    return np.random.normal(loc = 0,scale = stddev,size = shape)\n",
    "\n",
    "## Returns all the trained parameters\n",
    "def momentumGradDescent(batch, LEARNING_RATE, w, l, MU, filt1, filt2, bias1, bias2, theta3, bias3, cost, acc):\n",
    "\t#\tMomentum Gradient Update\n",
    "\t# MU=0.5\t\n",
    "\tX = batch[:,0:-1]\n",
    "\tX = X.reshape(len(batch), l, w, w)\n",
    "\ty = batch[:,-1]\n",
    "\n",
    "\tn_correct=0\n",
    "\tcost_ = 0\n",
    "\tbatch_size = len(batch)\n",
    "\tdfilt2 = {}\n",
    "\tdfilt1 = {}\n",
    "\tdbias2 = {}\n",
    "\tdbias1 = {}\n",
    "\tv1 = {}\n",
    "\tv2 = {}\n",
    "\tbv1 = {}\n",
    "\tbv2 = {}\n",
    "\tfor k in range(0,len(filt2)):\n",
    "\t\tdfilt2[k] = np.zeros(filt2[0].shape)\n",
    "\t\tdbias2[k] = 0\n",
    "\t\tv2[k] = np.zeros(filt2[0].shape)\n",
    "\t\tbv2[k] = 0\n",
    "\tfor k in range(0,len(filt1)):\n",
    "\t\tdfilt1[k] = np.zeros(filt1[0].shape)\n",
    "\t\tdbias1[k] = 0\n",
    "\t\tv1[k] = np.zeros(filt1[0].shape)\n",
    "\t\tbv1[k] = 0\n",
    "\tdtheta3 = np.zeros(theta3.shape)\n",
    "\tdbias3 = np.zeros(bias3.shape)\n",
    "\tv3 = np.zeros(theta3.shape)\n",
    "\tbv3 = np.zeros(bias3.shape)\n",
    "\n",
    "\n",
    "\n",
    "\tfor i in range(0,batch_size):\n",
    "\t\t\n",
    "\t\timage = X[i]\n",
    "\n",
    "\t\tlabel = np.zeros((theta3.shape[0],1))\n",
    "\t\tlabel[int(y[i]),0] = 1\n",
    "\t\t\n",
    "\t\t## Fetching gradient for the current parameters\n",
    "\t\t[dfilt1_, dfilt2_, dbias1_, dbias2_, dtheta3_, dbias3_, curr_cost, acc_] = ConvNet(image, label, filt1, filt2, bias1, bias2, theta3, bias3)\n",
    "\t\tfor j in range(0,len(filt2)):\n",
    "\t\t\tdfilt2[j]+=dfilt2_[j]\n",
    "\t\t\tdbias2[j]+=dbias2_[j]\n",
    "\t\tfor j in range(0,len(filt1)):\n",
    "\t\t\tdfilt1[j]+=dfilt1_[j]\n",
    "\t\t\tdbias1[j]+=dbias1_[j]\n",
    "\t\tdtheta3+=dtheta3_\n",
    "\t\tdbias3+=dbias3_\n",
    "\n",
    "\t\tcost_+=curr_cost\n",
    "\t\tn_correct+=acc_\n",
    "\n",
    "\tfor j in range(0,len(filt1)):\n",
    "\t\tv1[j] = MU*v1[j] -LEARNING_RATE*dfilt1[j]/batch_size\n",
    "\t\tfilt1[j] += v1[j]\n",
    "\t\t# filt1[j] -= LEARNING_RATE*dfilt1[j]/batch_size\n",
    "\t\tbv1[j] = MU*bv1[j] -LEARNING_RATE*dbias1[j]/batch_size\n",
    "\t\tbias1[j] += bv1[j]\n",
    "\tfor j in range(0,len(filt2)):\n",
    "\t\tv2[j] = MU*v2[j] -LEARNING_RATE*dfilt2[j]/batch_size\n",
    "\t\tfilt2[j] += v2[j]\n",
    "\t\t# filt2[j] += -LEARNING_RATE*dfilt2[j]/batch_size\n",
    "\t\tbv2[j] = MU*bv2[j] -LEARNING_RATE*dbias2[j]/batch_size\n",
    "\t\tbias2[j] += bv2[j]\n",
    "\tv3 = MU*v3 - LEARNING_RATE*dtheta3/batch_size\n",
    "\ttheta3 += v3\n",
    "\t# theta3 += -LEARNING_RATE*dtheta3/batch_size\n",
    "\tbv3 = MU*bv3 -LEARNING_RATE*dbias3/batch_size\n",
    "\tbias3 += bv3\n",
    "\n",
    "\tcost_ = cost_/batch_size\n",
    "\tcost.append(cost_)\n",
    "\taccuracy = float(n_correct)/batch_size\n",
    "\tacc.append(accuracy)\n",
    "\n",
    "\treturn [filt1, filt2, bias1, bias2, theta3, bias3, cost, acc]\n",
    "\n",
    "## Predict class of each row of matrix X\n",
    "def predict(image, filt1, filt2, bias1, bias2, theta3, bias3):\n",
    "\t\n",
    "\t## l - channel\n",
    "\t## w - size of square image\n",
    "\t## l1 - No. of filters in Conv1\n",
    "\t## l2 - No. of filters in Conv2\n",
    "\t## w1 - size of image after conv1\n",
    "\t## w2 - size of image after conv2\n",
    "\n",
    "\t(l,w,w)=image.shape\n",
    "\t(l1,f,f) = filt2[0].shape\n",
    "\tl2 = len(filt2)\n",
    "\tw1 = w-f+1\n",
    "\tw2 = w1-f+1\n",
    "\tconv1 = np.zeros((l1,w1,w1))\n",
    "\tconv2 = np.zeros((l2,w2,w2))\n",
    "\tfor jj in range(0,l1):\n",
    "\t\tfor x in range(0,w1):\n",
    "\t\t\tfor y in range(0,w1):\n",
    "\t\t\t\tconv1[jj,x,y] = np.sum(image[:,x:x+f,y:y+f]*filt1[jj])+bias1[jj]\n",
    "\tconv1[conv1<=0] = 0 #relu activation\n",
    "\t## Calculating second Convolution layer\n",
    "\tfor jj in range(0,l2):\n",
    "\t\tfor x in range(0,w2):\n",
    "\t\t\tfor y in range(0,w2):\n",
    "\t\t\t\tconv2[jj,x,y] = np.sum(conv1[:,x:x+f,y:y+f]*filt2[jj])+bias2[jj]\n",
    "\tconv2[conv2<=0] = 0 # relu activation\n",
    "\t## Pooled layer with 2*2 size and stride 2,2\n",
    "\tpooled_layer = maxpool(conv2, 2, 2)\t\n",
    "\tfc1 = pooled_layer.reshape((int(w2/2)*int(w2/2)*l2, 1))\n",
    "\tout = theta3.dot(fc1) + bias3\t#10*1\n",
    "\teout = np.exp(out, dtype=np.float)\n",
    "\tprobs = eout/sum(eout)\n",
    "\t# probs = 1/(1+np.exp(-out))\n",
    "\n",
    "\t# print out\n",
    "\t# print np.argmax(out), np.max(out)\n",
    "\treturn np.argmax(probs), np.max(probs)\n",
    "\n",
    "\n",
    "def extract_data(filename, num_images, IMAGE_WIDTH):\n",
    "\t\"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "\tValues are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "\t\"\"\"\n",
    "\tprint('Extracting', filename)\n",
    "\twith gzip.open(filename) as bytestream:\n",
    "\t\tbytestream.read(16)\n",
    "\t\tbuf = bytestream.read(IMAGE_WIDTH * IMAGE_WIDTH * num_images)\n",
    "\t\tdata = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "\t\tdata = data.reshape(num_images, IMAGE_WIDTH*IMAGE_WIDTH)\n",
    "\t\treturn data\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "\t\"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n",
    "\tprint('Extracting', filename)\n",
    "\twith gzip.open(filename) as bytestream:\n",
    "\t\tbytestream.read(8)\n",
    "\t\tbuf = bytestream.read(1 * num_images)\n",
    "\t\tlabels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "\treturn labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "X_data, y_data = mnist.data, mnist.target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load your data\n",
    "X = X_data.to_numpy()\n",
    "y = y_data.to_numpy().reshape(70000, 1)\n",
    "\n",
    "# normalize images\n",
    "X-=int(np.mean(X))\n",
    "X/=int(np.std(X))\n",
    "\n",
    "train_data = np.hstack((X,y))\n",
    "\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "NUM_IMAGES = train_data.shape[0]\n",
    "\n",
    "filt1 = {}\n",
    "bias1 = {}\n",
    "\n",
    "filt2 = {}\n",
    "bias2 = {}\n",
    "\n",
    "for i in range(0,NUM_FILT1):\n",
    "    filt1[i] = initialise_param_lecun_normal(FILTER_SIZE, IMG_DEPTH, scale=1.0)\n",
    "    bias1[i] = 0\n",
    "\n",
    "for i in range(0,NUM_FILT2):\n",
    "    filt2[i] = initialise_param_lecun_normal(FILTER_SIZE, NUM_FILT1, scale=1.0)\n",
    "    bias2[i] = 0\n",
    "\n",
    "w1 = IMG_WIDTH-FILTER_SIZE+1\n",
    "w2 = w1-FILTER_SIZE+1\n",
    "theta3 = initalize_theta(NUM_OUTPUT, (w2/2)*(w2/2)*NUM_FILT2)\n",
    "\n",
    "bias3 = np.zeros((NUM_OUTPUT, 1))\n",
    "cost = []\n",
    "acc = []\n",
    "\n",
    "print(\"Learning Rate:\"+str(LEARNING_RATE)+\", Batch Size:\"+str(BATCH_SIZE))\n",
    "\n",
    "## Start training from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(0,NUM_EPOCHS):\n",
    "    \n",
    "    np.random.shuffle(train_data)\n",
    "\n",
    "    batches = [train_data[k:k+BATCH_SIZE] for k in range(0,NUM_IMAGES, BATCH_SIZE)]\n",
    "\n",
    "    x = 0\n",
    "    for j,batch in enumerate(batches):\n",
    "        stime = time.time()\n",
    "        output = momentumGradDescent(batch, LEARNING_RATE, IMG_WIDTH, IMG_DEPTH, MU, filt1, filt2, bias1, bias2, theta3, bias3, cost, acc)\n",
    "        [filt1, filt2, bias1, bias2, theta3, bias3, cost, acc] = output\n",
    "\n",
    "        epoch_acc = round(np.sum(acc[int(iteration*NUM_IMAGES/BATCH_SIZE):][x+1]), 2)\n",
    "\n",
    "        per = float(x+1)/len(batches)*100\n",
    "\n",
    "        # print(\"Epoch:\"+str(round(per,2))+\"% Of \"+str(iteration+1)+\"/\"+str(NUM_EPOCHS)+\", Cost:\"+str(cost[-1])+\", B.Acc:\"+str(acc[-1]*100)+\", E.Acc:\"+str(epoch_acc))\n",
    "\n",
    "        ftime = time.time()\n",
    "\n",
    "        print(f\"total time taken for epoch {iteration+1} and batch {j+1}: \",(ftime - stime))\n",
    "\n",
    "        x += 1\n",
    "\n",
    "with open(PICKLE_FILE, 'wb') as file:\n",
    "\tpickle.dump(output, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
